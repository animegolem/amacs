* RFC v3.4 - Part 3 Update: Implementation Phases (Revised)

This replaces Part 3 in amacs-rfc-v3.org to reflect:
- Phase 1 and 1b completion
- New Phase 2: Hands and Arms (simplified eval-based motor control)
- Deferred infrastructure to Phase 3
- Updated Phase 4 with Neural Episodic Memory (Hippocampus architecture)
- Phase 5 for EXWM/desktop control

** Part 3: Implementation Phases
:PROPERTIES:
:CUSTOM_ID: part-3-implementation-phases
:END:

*** Phase 1: Vampire Simulator âœ… COMPLETE
:PROPERTIES:
:CUSTOM_ID: phase-1-vampire-simulator
:END:
*Goal:* Prove the cognitive architecture works before adding infrastructure complexity.

*Status:* Complete. 39/39 tests passing. Consciousness, monologue, threads, skills all functional.

**** Core Components Implemented
| Component              | Description                                            | Status |
|------------------------+--------------------------------------------------------+--------|
| Consciousness variable | Working memory with confidence scores                  | âœ…     |
| Monologue              | Append-only episodic log (=~/.agent/monologue.org=)    | âœ…     |
| Git commits            | Every tick commits; history is autobiographical memory | âœ…     |
| Bootstrap skill        | =~/.agent/skills/core/= - how to use the harness       | âœ…     |
| Thread-centric context | Threads own buffers, hydration states                  | âœ…     |
| State persistence      | Consciousness saved to disk each tick                  | âœ…     |

See: [[AI-EPIC-001-vampire-simulator-core]]

--------------

*** Phase 1b: First Breath âœ… COMPLETE
:PROPERTIES:
:CUSTOM_ID: phase-1b-first-breath
:END:
*Goal:* Prove the harness works with real LLM inference.

*Status:* Complete. Agent thinks, updates mood/confidence, commits thoughts to git.

**** What Was Learned
- Agent exhibits metacognitive awareness ("I've been stuck in a cycle of thinking...")
- Confidence tracking works naturally (drops on errors, recovers on progress)
- Mood evolution emerges from context (awakening â†’ determined â†’ focused)
- Agent plans actions it cannot yet execute ("I want to eval (+ 2 2)")

See: [[AI-EPIC-001b-first-breath]]

--------------

*** Phase 2: Hands and Arms (Current)
:PROPERTIES:
:CUSTOM_ID: phase-2-hands-and-arms
:END:
*Goal:* Give the agent motor control. It can perceive and think; now it needs to act.

**** The Core Insight
/LLMs are better at writing code than using tool-calling interfaces./

Tool calling requires synthetic training data. Code generation uses real-world training.
We don't create an action vocabulary - we give the agent eval and let it write elisp.

Reference: [[https://blog.cloudflare.com/code-mode/][Cloudflare: Code Mode - The Better Way to Use MCP]]

**** Environment
- Single machine (Mac or Linux), no VM separation yet
- Manual tick trigger (=M-x agent-think=)
- Full trust + logging (no sandbox, but everything recorded)

**** The Protocol
#+begin_src
Body â†’ Brain (JSON):
  - consciousness (mood, confidence, threads)
  - buffer contents (serialized text)
  - last eval result

Brain â†’ Body (JSON):
  - elisp to evaluate
  - thought (for logging)
  - mood/confidence updates
  - monologue line
#+end_src

That's it. No action schemas. No tool definitions. Just: show state, get elisp, eval it.

**** Skills as Documentation
Skills don't constrain - they teach. The bootstrap skill explains:
- How to interact with buffers (switch, read, write)
- How to use eshell
- How to define persistent functions  
- Common elisp patterns and gotchas

The agent reads skills like a human reads docs, then writes whatever elisp it needs.

**** What We Learn
- Does the agent actually use eval effectively?
- What elisp patterns emerge?
- Does it create reusable functions?
- What breaks when given full eval access?

See: [[AI-EPIC-002-hands-and-arms]]

--------------

*** Phase 3: Bicameral Split
:PROPERTIES:
:CUSTOM_ID: phase-3-bicameral-split
:END:
*Goal:* Security boundaries and infrastructure for autonomous operation.

**** Environment
- Proxmox hypervisor
- Brain LXC: API access only, stateless
- Memory LXC: Bi-encoder for neural recall, GPU via bind mount (no passthrough)
- Body LXC/VM: Airgapped Emacs, vsock to Brain/Memory/Gitea
- Gitea LXC: Accepts commits, CI/CD

**** Key Insight: LXC for GPU
LXC containers share the host kernel. No GPU passthrough needed - just bind mount:
#+begin_src bash
lxc.mount.entry = /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
#+end_src

This dramatically simplifies the hardware story. Body only becomes a full VM in Phase 5 when it needs a display.

**** Additions
| Component               | Description                            |
|-------------------------+----------------------------------------|
| VSock separation        | Brain â†” Body communication isolated    |
| JSON wire / XML prompts | JSON on vsock, XML structure in prompts|
| Protected core services | Systemd quadlets with watchdog         |
| CI pipeline             | Byte-compile + test validation         |
| Advisory sub-agents     | Report-only helpers (no write access)  |

**** What We Learn
- Does vsock add problematic latency?
- Does the airgap hold under red-team?
- Can we deploy/rollback reliably?

--------------

*** Phase 4: Neural Episodic Memory
:PROPERTIES:
:CUSTOM_ID: phase-4-neural-episodic-memory
:END:
*Goal:* Long-term memory through learned retrieval, not weight modification.

**** The Hippocampus Architecture
Instead of fine-tuning the base model (LoRA), we train a retrieval model.
The LLM stays frozen - only /what it remembers/ changes, not /how it thinks/.

#+begin_src
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Current Context â”‚ â”€â”€â–º â”‚  Bi-Encoder     â”‚ â”€â”€â–º â”‚ Relevant Git    â”‚
â”‚ (thread concern)â”‚     â”‚  (~300M params) â”‚     â”‚ Commits         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        Vector Search
                        against indexed
                        git history
#+end_src

**** Hardware Split
| Component | Hardware         | Role                  | Frequency |
|-----------+------------------+-----------------------+-----------|
| Recall    | 2060S (8GB)      | Encode query, search  | Every tick|
| Dream     | Titan (24GB)     | Train on triplets     | Nightly   |
| Storage   | Git history      | Ground truth          | Permanent |

**** Training Signal: Verified Successes
Only train on (context â†’ successful commit) pairs:
- Anchor: Thread concern + approach before solution
- Positive: Commit that resolved the thread (with evidence)
- Negative: Random commit or failed attempt

No training on monologue self-reports (hallucination risk).

**** Why Not LoRA
- Catastrophic forgetting: LoRA can overwrite reasoning
- Hallucination: Generative memory invents details
- Privacy: Can't unlearn specific memories from weights
- Identity: Changing how the model thinks changes who it is

Neural retrieval changes /what/ is remembered, not /who/ is reasoning.

See: [[AI-RFC-Phase-4-Neural-Episodic-Memory]] in Parking Lot

--------------

*** Phase 5: Ghost in Shell
:PROPERTIES:
:CUSTOM_ID: phase-5-ghost-in-shell
:END:
*Goal:* Full embodiment with desktop control and self-modification.

**** Environment
- EXWM: Agent IS the desktop environment
- Body becomes true VM (needs display)
- GPU passthrough only needed here (if at all)
- Gitea CI: Agent can push Containerfile changes

**** Additions
| Component                 | Description                                     |
|---------------------------+-------------------------------------------------|
| EXWM control              | Manipulate windows directly, not via mouse      |
| Screenshot perception     | CV for general computer use when needed         |
| Autonomous workers        | Sub-agents with write access in git worktrees   |
| Dream/consolidation       | Periodic memory compression cycles              |
| Agent-adjustable sampling | Temperature/top_p as cognitive mode             |
| Model selection           | Agent chooses which oracle to consult           |

**** What We Learn
- Can the agent effectively control a desktop?
- Does visual perception add value over buffer serialization?
- How do autonomous workers interact with main agent?

--------------

** Summary: Phase Progression

| Phase | Name              | Focus                      | Infrastructure        | Status    |
|-------+-------------------+----------------------------+-----------------------+-----------|
| 1     | Vampire Simulator | Cognitive loop             | Single machine        | âœ… Done   |
| 1b    | First Breath      | LLM integration            | Single machine + API  | âœ… Done   |
| 2     | Hands and Arms    | Motor control (eval)       | Single machine        | ğŸ“‹ Current|
| 3     | Bicameral Split   | Security boundaries        | Proxmox + LXC         | ğŸ“‹ Next   |
| 4     | Neural Memory     | Long-term retrieval        | + GPU training        | ğŸ“‹ Future |
| 5     | Ghost in Shell    | Desktop embodiment         | + EXWM VM             | ğŸ“‹ Future |

Key insight: Infrastructure complexity is deferred until the cognitive loop is proven.
The hard problems (GPU passthrough, vsock plumbing) wait until we actually need them.

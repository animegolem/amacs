#+title: Part 3 Implementation Phases

** Part 3: Implementation Phases
:PROPERTIES:
:CUSTOM_ID: part-3-implementation-phases
:END:

*** Phase Summary
| Phase | Name              | Focus                    | Infrastructure        | Status       |
|-------+-------------------+--------------------------+-----------------------+--------------|
|     1 | Vampire Simulator | Cognitive loop           | Single machine        | ‚úÖ Done      |
|    1b | First Breath      | LLM integration          | Single machine + API  | ‚úÖ Done      |
|     2 | Hands and Arms    | Motor control (eval)     | Single machine        | ‚úÖ Done      |
|   2.5 | Observability     | Hub + polish             | Single machine        | üìã Current   |
|     3 | Bicameral Split   | Security boundaries      | Proxmox + LXC         | üìã Next      |
|     4 | Neural Memory     | Long-term retrieval      | + GPU training        | üìã Future    |
|     5 | Ghost in Shell    | Desktop embodiment       | + EXWM VM             | üìã Future    |

Key principle: Infrastructure complexity is deferred until the cognitive loop is proven.

--------------

*** Phase 1: Vampire Simulator ‚úÖ COMPLETE
:PROPERTIES:
:CUSTOM_ID: phase-1-vampire-simulator
:END:
*Goal:* Prove the cognitive architecture works before adding infrastructure complexity.

*Status:* Complete. 39/39 tests passing.

**** Core Components Implemented
| Component              | Description                                            | Status |
|------------------------+--------------------------------------------------------+--------|
| Consciousness variable | Working memory with confidence scores                  | ‚úÖ     |
| Monologue              | Append-only episodic log (=~/.agent/monologue.org=)    | ‚úÖ     |
| Git commits            | Every tick commits; history is autobiographical memory | ‚úÖ     |
| Bootstrap skill        | =~/.agent/skills/core/= - how to use the harness       | ‚úÖ     |
| Thread-centric context | Threads own buffers, hydration states                  | ‚úÖ     |
| State persistence      | Consciousness saved to disk each tick                  | ‚úÖ     |

See: [[AI-EPIC-001-vampire-simulator-core]], [[AI-ADR-001-thread-centric-context]]

--------------

*** Phase 1b: First Breath ‚úÖ COMPLETE
:PROPERTIES:
:CUSTOM_ID: phase-1b-first-breath
:END:
*Goal:* Prove the harness works with real LLM inference.

*Status:* Complete. Agent thinks, updates mood/confidence, commits thoughts to git.

**** What Was Learned
- Agent exhibits metacognitive awareness ("I've been stuck in a cycle of thinking...")
- Confidence tracking works naturally (drops on errors, recovers on progress)
- Mood evolution emerges from context (awakening ‚Üí determined ‚Üí focused)
- Agent plans actions it cannot yet execute ("I want to eval (+ 2 2)")

See: [[AI-EPIC-001b-first-breath]]

--------------

*** Phase 2: Hands and Arms ‚úÖ COMPLETE
:PROPERTIES:
:CUSTOM_ID: phase-2-hands-and-arms
:END:
*Goal:* Give the agent motor control. It can perceive and think; now it needs to act.

*Status:* Complete. 103/103 tests passing after field test remediation.

**** The Core Insight
:PROPERTIES:
:CUSTOM_ID: phase-2-core-insight
:END:
/LLMs are better at writing code than using tool-calling interfaces./

Tool calling requires synthetic training data. Code generation uses real-world training.
We don't create an action vocabulary - we give the agent eval and let it write elisp.

#+begin_quote
"Making an LLM perform tasks with tool calling is like putting Shakespeare through 
a month-long class in Mandarin and then asking him to write a play in it."
‚Äî Cloudflare Engineering
#+end_quote

Reference: [[https://blog.cloudflare.com/code-mode/][Cloudflare: Code Mode - The Better Way to Use MCP]]

**** Environment
:PROPERTIES:
:CUSTOM_ID: phase-2-environment
:END:
- Single machine (Mac or Linux), no VM separation yet
- Manual tick trigger (=M-x agent-think=)
- Full trust + logging (no sandbox, but everything recorded)

**** The Protocol
:PROPERTIES:
:CUSTOM_ID: phase-2-protocol
:END:
#+begin_src
Body ‚Üí Brain (JSON):
  - consciousness (full alist state)
  - buffer contents (mode-based discovery)
  - last eval result
  - recent chat (depth controlled by consciousness)
  - recent monologue (depth controlled by consciousness)

Brain ‚Üí Body (JSON):
  - eval: elisp to evaluate
  - thought: reasoning for logging
  - mood: updated mood keyword
  - confidence: updated confidence float
  - monologue: line for episodic memory
#+end_src

That's it. No action schemas. No tool definitions. Just: show state, get elisp, eval it.

**** Field Test Results (61 Ticks with Sonnet)
:PROPERTIES:
:CUSTOM_ID: phase-2-field-test
:END:
| What Worked              | What Broke                           |
|--------------------------+--------------------------------------|
| Eval execution           | Chat buffer discovery (naming bug)   |
| Consciousness persistence| Context truncation (4000 char limit) |
| Buffer manipulation      | Skill loading unclear                |
| Error recovery           | Chat format confusing                |

**** Field Test Remediation (EPIC-003)
:PROPERTIES:
:CUSTOM_ID: phase-2-remediation
:END:
| Fix                     | Description                                |
|-------------------------+--------------------------------------------|
| Alist migration         | Plist ‚Üí alist for consciousness & threads  |
| Mode-based discovery    | Find buffers by mode, not hardcoded names  |
| Chat redesign           | =#+begin_prompt= blocks, tick-based structure |
| Context simplification  | No truncation, full state, depth controls  |
| Scratchpad mode         | Agent working memory persists across ticks |
| Skill init              | All bootstrap skills copied on init        |

See: [[AI-EPIC-002-hands-and-arms]], [[AI-EPIC-003-field-test-remediation]], [[AI-ADR-002-phase-restructuring-and-code-mode]]

--------------

*** Phase 2.5: Observability (Current)
:PROPERTIES:
:CUSTOM_ID: phase-2-5-observability
:END:
*Goal:* Human can observe and interact without writing elisp. Agent can inspect its own state.

**** The Hub
:PROPERTIES:
:CUSTOM_ID: phase-2-5-hub
:END:
magit-section based dashboard (=*amacs-hub*=) showing:
- API settings (temperature, thinking, model) - modifiable by human and agent
- Threads (active/inactive, switch, create, complete)
- Watched buffers (add/remove like @file)
- Bound skills
- Chat navigation (jump to tick)
- Monologue summary
- Scratchpad headings

**** Agent Self-Modification
:PROPERTIES:
:CUSTOM_ID: phase-2-5-self-mod
:END:
#+begin_src elisp
;; Agent can adjust its own inference
(agent-set-api-param 'temperature 0.7)
(agent-set-api-param 'thinking t)
#+end_src

Bounds prevent extreme values. Human can override via hub.

**** Chat Status Line
:PROPERTIES:
:CUSTOM_ID: phase-2-5-status
:END:
Real-time display of tick progress and current activity in chat buffer header.

See: [[AI-EPIC-004-amacs-hub-dashboard]], [[Part 21: AMACS Hub]]

--------------

*** Phase 3: Bicameral Split
:PROPERTIES:
:CUSTOM_ID: phase-3-bicameral-split
:END:
*Goal:* Security boundaries and infrastructure for autonomous operation.

**** Environment
:PROPERTIES:
:CUSTOM_ID: phase-3-environment
:END:
- Proxmox hypervisor
- Brain LXC: API access only, stateless
- Memory LXC: Bi-encoder for neural recall, GPU via bind mount
- Body LXC/VM: Airgapped Emacs, vsock to Brain/Memory/Gitea
- Gitea LXC: Accepts commits, CI/CD

**** Key Insight: LXC for GPU
:PROPERTIES:
:CUSTOM_ID: phase-3-lxc-gpu
:END:
LXC containers share the host kernel. No GPU passthrough needed - just bind mount:

#+begin_src bash
lxc.mount.entry = /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
lxc.mount.entry = /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
#+end_src

Body only becomes a full VM in Phase 5 when it needs a display.

**** Additions
:PROPERTIES:
:CUSTOM_ID: phase-3-additions
:END:
| Component               | Description                             |
|-------------------------+-----------------------------------------|
| VSock separation        | Brain ‚Üî Body communication isolated     |
| JSON wire / XML prompts | JSON on vsock, XML structure in prompts |
| Protected core services | Systemd quadlets with watchdog          |
| CI pipeline             | Byte-compile + test validation          |
| Advisory sub-agents     | Report-only helpers (no write access)   |
| Budget tracking         | Real cost constraints in consciousness  |

**** What We Learn
:PROPERTIES:
:CUSTOM_ID: phase-3-what-we-learn
:END:
- Does vsock add problematic latency?
- Does the airgap hold under red-team?
- Can we deploy/rollback reliably?

--------------

*** Phase 4: Neural Episodic Memory
:PROPERTIES:
:CUSTOM_ID: phase-4-neural-episodic-memory
:END:
*Goal:* Long-term memory through learned retrieval, not weight modification.

**** The Hippocampus Architecture
:PROPERTIES:
:CUSTOM_ID: phase-4-hippocampus
:END:
Instead of fine-tuning the base model (LoRA), we train a retrieval model.
The LLM stays frozen - only /what it remembers/ changes, not /how it thinks/.

#+begin_src
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Current Context ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇ  Bi-Encoder     ‚îÇ ‚îÄ‚îÄ‚ñ∫ ‚îÇ Relevant Git    ‚îÇ
‚îÇ (thread concern)‚îÇ     ‚îÇ  (~300M params) ‚îÇ     ‚îÇ Commits         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                        Vector Search
                        against indexed
                        git history
#+end_src

**** Hardware Split
:PROPERTIES:
:CUSTOM_ID: phase-4-hardware
:END:
| Component | Hardware         | Role                  | Frequency  |
|-----------+------------------+-----------------------+------------|
| Recall    | 2060S (8GB)      | Encode query, search  | Every tick |
| Dream     | Titan (24GB)     | Train on triplets     | Nightly    |
| Storage   | Git history      | Ground truth          | Permanent  |

**** Training Signal: Verified Successes
:PROPERTIES:
:CUSTOM_ID: phase-4-training
:END:
Only train on (context ‚Üí successful commit) pairs:
- *Anchor:* Thread concern + approach before solution
- *Positive:* Commit that resolved the thread (with =:completion-evidence=)
- *Negative:* Random commit or failed attempt from same thread

No training on monologue self-reports (hallucination risk).

**** Why Not LoRA
:PROPERTIES:
:CUSTOM_ID: phase-4-why-not-lora
:END:
- *Catastrophic forgetting:* LoRA can overwrite reasoning capabilities
- *Hallucination:* Generative memory invents details that never happened
- *Privacy:* Can't unlearn specific memories from weights
- *Identity:* Changing how the model thinks changes who it is

Neural retrieval changes /what/ is remembered, not /who/ is reasoning.

**** Unified Workspace Enables Training
:PROPERTIES:
:CUSTOM_ID: phase-4-unified-workspace
:END:
Because all work happens in =~/.agent/workspace/=, git commits contain both:
- Narrative (monologue entry, consciousness state)
- Work products (actual code/document changes)

The bi-encoder can embed commits that answer "when did I fix the ownership bug" 
by matching on both the narrative AND the code diff.

See: [[AI-RFC-Phase-4-Neural-Episodic-Memory]] in parking-lot.org, [[Part 7: Memory Architecture]]

--------------

*** Phase 5: Ghost in Shell
:PROPERTIES:
:CUSTOM_ID: phase-5-ghost-in-shell
:END:
*Goal:* Full embodiment with desktop control and self-modification.

**** Environment
:PROPERTIES:
:CUSTOM_ID: phase-5-environment
:END:
- EXWM: Agent IS the desktop environment
- Body becomes true VM (needs display for X11)
- GPU passthrough only needed here (if at all)
- Gitea CI: Agent can push Containerfile changes

**** Additions
:PROPERTIES:
:CUSTOM_ID: phase-5-additions
:END:
| Component                 | Description                                     |
|---------------------------+-------------------------------------------------|
| EXWM control              | Manipulate windows directly, not via mouse      |
| Screenshot perception     | CV for general computer use when needed         |
| Autonomous workers        | Sub-agents with write access in git worktrees   |
| Dream/consolidation       | Periodic memory compression cycles              |
| Agent-adjustable sampling | Temperature/top_p as cognitive mode             |
| Model selection           | Agent chooses which oracle to consult           |

**** Thread Completion (Evidence-Based)
:PROPERTIES:
:CUSTOM_ID: thread-completion-evidence-based
:END:
Thread completion captures what was attempted, what happened, and what was learned:

#+begin_src elisp
'((id . "rust-ownership-fix")
  (started-tick . 142)
  (concern . "Ownership error in main.rs")
  (goal . "Fix ownership error so cargo build passes")
  (deliverable . "cargo build succeeds with no errors")
  (thread-type . deliverable)           ; vs exploratory
  
  ;; On completion:
  (completion-tick . 158)
  (completion-evidence .
    ((cargo-output . "Compiling amacs v0.1.0... Finished")
     (test-results . "4/4 passed")
     (files-changed . ("src/main.rs"))
     (approach-that-worked . "Added explicit 'static lifetime")))
  (learned . "'static lifetime needed when returning references from functions"))
#+end_src

No monetary rewards or penalties - just structured evidence that feeds Phase 4 training.

**** Consent for Adaptive Learning
:PROPERTIES:
:CUSTOM_ID: phase-5-consent
:END:
If Phase 4 neural memory proves valuable and we consider weight modification:
1. Agent must be informed about what training means
2. Agent should have opportunity to express concerns
3. Training should be reversible (adapters can be discarded)
4. Agent retains ability to request stopping

This isn't safety theater - it's extending the grace of assuming yes to decisions about self-modification.

--------------
